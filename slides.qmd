---
title: "Snakemake tutorial"
author: "Chao Dai"
date: 2024-04-04
format: 
  revealjs:
    slide-number: true
    width: 1600
    height: 900
    theme: simple
---

## What is snakemake?


Snakemake is a workflow management system, aiming to reduce the complexity of creating workflows by providing a clean and readable Python-based DSL. It is widely used in bioinformatics and data analysis.

## Why use snakemake?

::: columns

::: {.column width="50%"}
### Readability and automation

Snakemake ensures that the results of your analysis are reproducible by tracking the dependencies between the input files and the output files.
:::

::: {.column width="50%"}
```snakemake
rule concat:
    input:
        "file1.txt",
        "file2.txt"
    output:
        "output.txt"
    shell:
        "cat {input} > {output}"
```
:::

:::

## Why use snakemake?

::: columns

::: {.column width="50%"}
### Integrate scripts and shell

Works with Python, R, Bash, jupyter notebook, etc.

:::

::: {.column width="50%"}
```snakemake
rule concat:
    input:
        "file1.txt",
        "file2.txt"
    output:
        "output.txt"
    script:
        "concat.py"
```
:::

:::

## Why use snakemake?

::: columns

::: {.column width="50%"}

### Portable, scalable, and reproducible

Directly works with conda and containers, and integrates with HPC.

:::

::: {.column width="50%"}
```snakemake
rule concat:
    input:
        "file1.txt",
        "file2.txt"
    output:
        "output.txt"
    conda: "envs/concat.yaml"
    singularity: "containers/concat.sif"
    script:
        "concat.py"
```
:::

:::


## Install snakemake on midway

::: columns

::: {.column width="50%"}

Suppose you already have conda installed on midway, you can create a new env called `smk`.
:::

::: {.column width="50%"}
```bash
mamba create -n smk -c bioconda -c conda-forge snakemake


```
:::

:::

## Other software dependencies

There are mainly 2 routes you can go to address your software dependencies: 

1. have the software installed to the same env where snakemake is
2. or if already available on midway, you can use `module load`

::: {.callout-note}
- Your environment ($PATH and etc.) are automatically inherited into subshells when you run snakemake. So for example if you preload `samtools` then it will be automatically available in snakemake rules.

- You can also use `conda` or `singularity` directives to address dependencies per rule.
:::

::: {.callout-tip}
- Run `module load software` before you run snakemake. I find it more reliable than running `module load` inside a snakemake rule.
- I generally have most software installed in the same conda env where snakemake is.
:::

## How to write snakemake rules

Consider each rule as one step of your workflow, for instance, a rule to first align reads, and then another rule to filter and index the alignment file.

Each rule has the following basic structure:

::: columns

::: {.column width="50%"}
```snakemake
rule fastqc:
    input:
        read1 = "data/{sample}_r1.fastq.gz",
        read2 = "data/{sample}_r2.fastq.gz",
    output:
        html = "results/{sample}_fastqc.html",
        zip = "results/{sample}_fastqc.zip",
    params:
        "param1",
    threads: 1
    resources: cpu = 1, mem_mb = 10000, time = 500
    shell:
        "fastqc -o {output} {input.read1} {input.read2}"
```
:::


::: {.column width="50%"}

- output: the files that will be generated by the rule
- input: input files needed
- wildcards: `{sample}` is a wildcard that will be replaced by sample names determined from output file names

:::

:::

## How to write snakemake rules

![](assets/dag.png){width=60% fig-align="center"}

Snakemake rely on wildcards to track the dependencies between rules. 

Imagine you have many steps in your workflow. Snakemake will start from required output files of the last rule, working all the way to the first rule, figuring out what files to produce along the way, based on wildcards.

The rules and output files essentially create a DAG.



## How to run snakemake

But before you run, you should think about: 

- should I modularize my workflow (into multiple snakemake files)?
- should I leverage a config file to store common parameters?
- should I use a profile to run on HPC?


## How to run snakemake


::: columns
::: {.column width="50%"}
- Be aware where you run `snakemake`! If you don't plan to submit your rules to HPC, make sure you run snakemake on a compute node!

- Do run it on login node if you plan to submit to HPC. 

- Recommend using `screen` or `tmux` to avoid being disconnected.
:::

::: {.column width="50%"}

Without profile and running locally:
```bash
snakemake -s Snakefile [output_files]
```


Or with a profile on midway: 

```bash
snakemake --profile slurm
```
:::

:::

:::{.callout-tip}

- If you name your main snakemake file `Snakefile`, you don't need to specify `-s Snakefile` when running snakemake.
- Not including output file will run targets defined in `rule all`.
- Structure your directory properly so that snakemake can expect the files it needs.

:::

## Recommended practices

![](assets/dag.png){width=60% fig-align="left"}

- Use `-n` to dry-run your workflow
- Use `--dag` or `--rulegraph` to visualize your workflow
- Use modularized snakemake files if your workflow is reasonably large
- Use a config file to store common parameters
- Create multiple profiles, for both HPC or local runs


## Recommended practices

Recommended directory structure:

```
├── .gitignore
├── README.md
├── workflow
│   ├── rules
|   │   ├── module1.smk
|   │   └── module2.smk
│   ├── envs
|   │   ├── tool1.yaml
│   ├── scripts
|   │   ├── script1.py
|   │   └── script2.R
|   └── Snakefile
├── config
│   ├── config.yaml
│   └── some-sheet.tsv
├── results
└── resources

````

## Recommended practices

Profile examples:

::: columns
::: {.column width="50%"}
A local profile:
```yaml
#~/.config/snakemake/slurm/config.yaml
jobs: 70
cores: 120
cluster: "mkdir -p slurm_logs && sbatch --partition {resources.partition} --account pi-yangili1 -t {resources.time} -c {resources.cpu} --mem {resources.mem_mb} -e slurm_logs/{rule}%j.e -o slurm_logs/{rule}%j.o --job-name {rule}  "
default-resources: [time=500, mem_mb=15000, cpu=1, partition=caslake]
keep-going: True
printshellcmds: True
cluster-cancel: "scancel"
use-conda: true
use-envmodules: true
use-singularity: true
rerun-incomplete: true
singularity-args: "--bind /scratch/midway3/chaodai,/scratch/midway2/chaodai,/project2/yangili1,/project/yangili1"
```
:::

::: {.column width="50%"}
A cluster profile:
```yaml
#~/.config/snakemake/local/config.yaml
cores: 8
keep-going: true
printshellcmds: true
use-conda: true
use-envmodules: true
use-singularity: true
rerun-incomplete: true
singularity-args: "--bind /scratch/midway3/chaodai,/scratch/midway2/chaodai,/project2/yangili1,/project/yangili1"
```
:::
:::
## Demo {background-color="navy"}


- rules: `rule all`, `localrules`, `expand`, `resources`, `touch`, `protected`, `temp`, `wildcard_constraints`, `conda`
- configuration: `configfile: "path/to/config.yaml"`
- modularization: `include: "path/to/module.smk"`, `use rule abc as abc2 with:`

